{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a868f20",
   "metadata": {},
   "source": [
    "# Feature Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b33a9",
   "metadata": {},
   "source": [
    "### train.csv\n",
    "\n",
    "the train set, containing the user ids and whether they have churned.\n",
    "\n",
    "msno: user id\n",
    "\n",
    "is_churn: This is the target variable. Churn is defined as whether the user did not continue the subscription within 30 days of expiration. is_churn = 1 means churn,is_churn = 0 means renewal.\n",
    "\n",
    "### transactions.csv\n",
    "transactions of users up until 2/28/2017.\n",
    "\n",
    "msno: user id\n",
    "\n",
    "payment_method_id: payment method\n",
    "\n",
    "payment_plan_days: length of membership plan in days\n",
    "\n",
    "plan_list_price: in New Taiwan Dollar (NTD)\n",
    "\n",
    "actual_amount_paid: in New Taiwan Dollar (NTD)\n",
    "\n",
    "is_auto_renew\n",
    "\n",
    "transaction_date: format %Y%m%d\n",
    "\n",
    "membership_expire_date: format %Y%m%d\n",
    "\n",
    "is_cancel: whether or not the user canceled the membership in this transaction.\n",
    "\n",
    "\n",
    "### user_logs.csv\n",
    "daily user logs describing listening behaviors of a user. Data collected until 2/28/2017.\n",
    "\n",
    "msno: user id\n",
    "\n",
    "date: format %Y%m%d\n",
    "\n",
    "num_25: # of songs played less than 25% of the song length\n",
    "\n",
    "num_50: # of songs played between 25% to 50% of the song length\n",
    "\n",
    "num_75: # of songs played between 50% to 75% of of the song length\n",
    "\n",
    "num_985: # of songs played between 75% to 98.5% of the song length\n",
    "\n",
    "num_100: # of songs played over 98.5% of the song length\n",
    "\n",
    "num_unq: # of unique songs played\n",
    "\n",
    "total_secs: total seconds played\n",
    "    \n",
    "    \n",
    "### members.csv\n",
    "user information. Note that not every user in the dataset is available.\n",
    "\n",
    "msno\n",
    "\n",
    "city\n",
    "\n",
    "bd: age. Note: this column has outlier values ranging from -7000 to 2015, please use your judgement.\n",
    "\n",
    "gender\n",
    "\n",
    "registered_via: registration method\n",
    "\n",
    "registration_init_time: format %Y%m%d\n",
    "\n",
    "expiration_date: format %Y%m%d, taken as a snapshot at which the member.csv is extracted. Not representing the actual churn \n",
    "behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e262504",
   "metadata": {},
   "source": [
    "### Impoting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d38e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burzi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Numerical computing\n",
    "import pandas as pd # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt # Data visualization\n",
    "import seaborn as sns # Advanced data visualization\n",
    "import sklearn # Machine learning algorithms and tools\n",
    "import tensorflow as tf # Deep learning framework\n",
    "import keras # High-level neural networks API\n",
    "import statsmodels.api as sm # Statistical modeling\n",
    "import scipy.stats as stats # Scientific computing\n",
    "import plotly.express as px # Interactive data visualization\n",
    "import nltk # Natural language processing\n",
    "import xgboost as xgb # Gradient boosting library\n",
    "import lightgbm as lgb # Gradient boosting framework\n",
    "import catboost as cb # Gradient boosting on decision trees\n",
    "import imblearn # Library for handling imbalanced datasets\n",
    "# import eli5 # Explanation of machine learning models\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import shap # Interpretability and explainability of models\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, roc_curve, auc\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4b0bb",
   "metadata": {},
   "source": [
    "### Datasets (CSV Files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00254d8f",
   "metadata": {},
   "source": [
    "### Join all the dataframes into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890e4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_and_join_csv_files(file_paths, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Read multiple CSV files efficiently using chunking with Pandas and join them into one DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - file_paths (dict): Dictionary containing file paths for each CSV file.\n",
    "    - chunk_size (int, optional): Number of rows to read at a time. Default is 10,000.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Joined DataFrame containing data from all CSV files.\n",
    "    \"\"\"\n",
    "    # Initialize empty dictionary to store DataFrames for each CSV file\n",
    "    dfs = {}\n",
    "    \n",
    "    # Iterate over each file path and read CSV files in chunks\n",
    "    for key, file_path in file_paths.items():\n",
    "        # Initialize empty list to store chunks for the current DataFrame\n",
    "        chunks = []\n",
    "        \n",
    "        # Iterate over chunks for the current DataFrame\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            # Append the chunk to the list of chunks for the current DataFrame\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Concatenate the chunks for the current DataFrame into a single DataFrame\n",
    "        df = pd.concat(chunks)\n",
    "        \n",
    "        # Store the DataFrame in the dictionary with the corresponding key\n",
    "        dfs[key] = df\n",
    "    \n",
    "    # Join DataFrames\n",
    "    joined_df = dfs[\"train\"].merge(dfs[\"members\"], on='msno', how='left')\n",
    "    joined_df = joined_df.merge(dfs[\"transactions\"], on='msno', how='left')\n",
    "    joined_df = joined_df.merge(dfs[\"logs\"], on='msno', how='left')\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974f4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "file_paths = {\n",
    "    \"members\": \"data/members_v3.csv\",\n",
    "    \"train\": \"data/train_v2.csv\",\n",
    "    \"transactions\": \"data/transactions_v2.csv\",\n",
    "    \"logs\": \"data/user_logs_v2.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b35e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data_frame = read_and_join_csv_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    '''\n",
    "    This function takes a pandas dataframe as input, performs preprocessing (like dropping rows with NaN values, etc.),\n",
    "    and then returns a pandas dataframe.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed DataFrame.\n",
    "    '''\n",
    "    print(\"Preprocessing...\")\n",
    "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Drop rows with NaN values in any column\n",
    "    data = data.dropna(how='any')\n",
    "    print(\"Dropped rows with NaN values.\")\n",
    "\n",
    "    # Replace 'male' with 1 and 'female' with 2 in 'gender'\n",
    "    gender_mapping = {'male': 1, 'female': 2}\n",
    "    data['gender'] = data['gender'].map(gender_mapping)\n",
    "\n",
    "    # Convert float date to datetime for date columns\n",
    "    date_columns = ['registration_init_time', 'transaction_date', 'membership_expire_date', 'date']\n",
    "    for col in date_columns:\n",
    "        data[col] = pd.to_datetime(data[col], format='%Y%m%d', errors='ignore')\n",
    "    print(\"Converted float date columns to datetime.\")\n",
    "    \n",
    "    # Drop the 'msno' column\n",
    "    #data = data.drop(columns=['msno'])\n",
    "\n",
    "    # Reorder columns to move 'is_churn' towards the end\n",
    "    if 'is_churn' in data.columns:\n",
    "        churn_column = data.pop('is_churn')\n",
    "        data['is_churn'] = churn_column\n",
    "\n",
    "    print(\"Complete!\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the undersampled DataFrame 'undersampled_df' using the 'preprocessing' function,\n",
    "# and store the preprocessed DataFrame in 'preprocessed_df'.\n",
    "preprocessed_df = preprocessing(joined_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ab585",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plot(data, columns):\n",
    "    \"\"\"\n",
    "    Plot count plots for specified columns using matplotlib in separate windows.\n",
    "    \n",
    "    Args:\n",
    "    - data (pandas.DataFrame): The DataFrame containing the data.\n",
    "    - columns (list): List of columns to plot.\n",
    "    \"\"\"\n",
    "    print(\"Plotting count plots...\")\n",
    "    \n",
    "    # Plot count plots for each column\n",
    "    for col in columns:\n",
    "        # Create a new figure for each plot with larger horizontal size\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Create count plot\n",
    "        sns.countplot(x=col, data=data, palette='Set2')\n",
    "        plt.title(f'Count Plot of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    # Show plots\n",
    "    plt.show()\n",
    "    print(\"Plots completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fda2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame and 'columns' is a list of columns you want to plot\n",
    "columns_to_plot = ['city', 'gender', 'registered_via', 'payment_method_id', 'is_auto_renew', 'is_cancel', 'is_churn']\n",
    "count_plot(preprocessed_df, columns_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_is_churn(data):\n",
    "    \"\"\"\n",
    "    Plot the distribution of the 'is_churn' column and check the ratio.\n",
    "\n",
    "    Args:\n",
    "    - data (pandas.DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    # Count plot for 'is_churn'\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.countplot(x='is_churn', data=data, palette='Set2')\n",
    "    plt.title('Distribution of is_churn')\n",
    "    plt.xlabel('is_churn')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Add count labels\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate churn ratio\n",
    "    churn_ratio = data['is_churn'].value_counts(normalize=True) * 100\n",
    "    print(\"Churn ratio:\")\n",
    "    print(churn_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "plot_is_churn(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea0f7f",
   "metadata": {},
   "source": [
    "The proportion of churned customers to those who haven't churned is imbalanced. To address this, we have the option to either undersample or oversample the data. Given that we have a sufficient number of data points for model training, undersampling the data seems like a viable approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2fb97a",
   "metadata": {},
   "source": [
    "# Undersampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_data_quantity(data: pd.DataFrame, churn_percent: float, data_size_percent: float = 100.0) -> pd.DataFrame:\n",
    "    '''\n",
    "    Adjust the quantity of not churned data points while keeping churned data points unchanged.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input DataFrame.\n",
    "        churn_percent (float): The percentage of churned data points in the output DataFrame.\n",
    "        data_size_percent (float, optional): The percentage of data to be used for adjustment. Default is 100.0 (use all data).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The adjusted DataFrame.\n",
    "    '''\n",
    "    print(\"Adjusting data quantity...\")\n",
    "    \n",
    "    # Convert data_size_percent from percentage to fraction\n",
    "    data_size = data_size_percent / 100.0\n",
    "    \n",
    "    # Apply data size adjustment\n",
    "    data = data.sample(frac=data_size, random_state=0)\n",
    "    \n",
    "    # Calculate the number of churned and not churned data points\n",
    "    churned_count = data['is_churn'].sum()\n",
    "    not_churned_count = len(data) - churned_count\n",
    "    \n",
    "    # Calculate the desired number of not churned data points based on the percentage\n",
    "    desired_not_churned_count = int(churned_count / (churn_percent / 100))\n",
    "    \n",
    "    # If the desired count is less than the current count, sample a subset of not churned data points\n",
    "    if desired_not_churned_count < not_churned_count:\n",
    "        # Sample a subset of not churned data points\n",
    "        not_churned_data = data[data['is_churn'] == 0].sample(n=desired_not_churned_count, replace=False)\n",
    "        churned_data = data[data['is_churn'] == 1]  # Keep churned data points unchanged\n",
    "    else:\n",
    "        # Repeat not churned data points to match the desired count\n",
    "        repeat_factor = desired_not_churned_count // not_churned_count\n",
    "        remainder = desired_not_churned_count % not_churned_count\n",
    "        not_churned_data = pd.concat([data[data['is_churn'] == 0]] * repeat_factor)\n",
    "        if remainder > 0:\n",
    "            not_churned_data = pd.concat([not_churned_data, data[data['is_churn'] == 0].sample(n=remainder, replace=False)])\n",
    "        churned_data = data[data['is_churn'] == 1]  # Keep churned data points unchanged\n",
    "    \n",
    "    # Concatenate churned and adjusted not churned data points\n",
    "    adjusted_data = pd.concat([churned_data, not_churned_data])\n",
    "    \n",
    "    print(\"Data quantity adjustment complete!\")\n",
    "    \n",
    "    # Calculate and print the ratio of churned to not churned data points\n",
    "    ratio = churned_count / desired_not_churned_count\n",
    "    print(f\"Ratio of churned to not churned data points: {ratio:.2f}\")\n",
    "    \n",
    "    # Print value counts after adjustment\n",
    "    print(\"Value counts after adjustment:\\n\", adjusted_data['is_churn'].value_counts())\n",
    "    \n",
    "    # Plot count of each class\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    adjusted_data['is_churn'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
    "    plt.title('Count of Each Class')\n",
    "    plt.xlabel('is_churn')\n",
    "    plt.ylabel('Count')\n",
    "    for i, value in enumerate(adjusted_data['is_churn'].value_counts()):\n",
    "        plt.text(i, value, str(value), ha='center', va='bottom')\n",
    "    \n",
    "    # Plot ratio of class 1 to class 0\n",
    "    plt.subplot(1, 2, 2)\n",
    "    adjusted_data['is_churn'].value_counts(normalize=True).plot(kind='bar', color=['green', 'red'])\n",
    "    plt.title('Ratio of Class 1 to Class 0')\n",
    "    plt.xlabel('is_churn')\n",
    "    plt.ylabel('Ratio')\n",
    "    for i, value in enumerate(adjusted_data['is_churn'].value_counts(normalize=True)):\n",
    "        plt.text(i, value, f\"{value:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return adjusted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_df = adjust_data_quantity(preprocessed_df, churn_percent = 50, data_size_percent = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a882007",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a31b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    '''\n",
    "    Perform feature engineering on the provided DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input DataFrame containing the original features.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with additional engineered features.\n",
    "    '''\n",
    "\n",
    "    # Extract year, month, and day from registration_init_time\n",
    "    data['registration_year'] = data['registration_init_time'].dt.year\n",
    "    data['registration_month'] = data['registration_init_time'].dt.month\n",
    "    data['registration_day'] = data['registration_init_time'].dt.day\n",
    "\n",
    "    # Calculate subscription duration\n",
    "    data['subscription_duration'] = (data['membership_expire_date'] - data['registration_init_time']).dt.days\n",
    "\n",
    "    # Extract month and day of the week from date\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "    # Calculate listening session frequency\n",
    "    session_count = data.groupby('msno')['date'].count().reset_index()\n",
    "    session_count.columns = ['msno', 'session_count']\n",
    "    data = pd.merge(data, session_count, on='msno', how='left')\n",
    "\n",
    "    # Calculate average listening time per session\n",
    "    data['avg_listen_time_per_session'] = data['total_secs'] / data['session_count']\n",
    "\n",
    "    # Calculate ratio of skipped songs\n",
    "    data['skipped_ratio'] = (data['num_25'] + data['num_50']) / data['num_unq']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'featurization' function to preprocess the DataFrame 'preprocessed_df' \n",
    "# by creating additional features based on existing data, and store the result in 'featurized'.\n",
    "featurized_df = feature_engineering(undersampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_v2(data):\n",
    "    '''\n",
    "    Perform preprocessing on the provided DataFrame by dropping specified columns and rearranging the columns.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed DataFrame.\n",
    "    '''\n",
    "\n",
    "    # Drop specified columns\n",
    "    columns_to_drop = [\"msno\", \"registration_init_time\", \"transaction_date\", \"membership_expire_date\", \"date\"]\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Rearrange columns to place \"is_churn\" at the end\n",
    "    churn_column = data.pop(\"is_churn\")\n",
    "    data[\"is_churn\"] = churn_column\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_v2_df = preprocessing_v2(featurized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d4a20",
   "metadata": {},
   "source": [
    "# Outliers and Extra Large Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ad9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Drop rows containing outliers in each column of a DataFrame using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame to drop outliers from.\n",
    "    - threshold: float, optional (default=1.5)\n",
    "        The threshold multiplier for determining outliers. A higher threshold will result in fewer outliers being detected.\n",
    "\n",
    "    Returns:\n",
    "    - df_cleaned: DataFrame\n",
    "        A new DataFrame with rows containing outliers removed.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df = drop_outliers(preprocessed_v2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_large_values(X, large_threshold=1e6):\n",
    "    \"\"\"\n",
    "    Drop rows containing very large values from the feature matrix X.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy.ndarray\n",
    "        The feature matrix.\n",
    "    - large_threshold: float, optional (default=1e6)\n",
    "        The threshold for defining very large values.\n",
    "\n",
    "    Returns:\n",
    "    - X_cleaned: numpy.ndarray\n",
    "        The feature matrix with rows containing very large values removed.\n",
    "    \"\"\"\n",
    "    # Check for very large values\n",
    "    large_rows = np.any(np.abs(X) > large_threshold, axis=1)\n",
    "    if np.any(large_rows):\n",
    "        print(\"Rows with very large values found in X. Dropping...\")\n",
    "        X_cleaned = X[~large_rows]  # Drop rows with large values\n",
    "    else:\n",
    "        X_cleaned = X.copy()  # If no large values found, return a copy of X\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_val_df = drop_rows_with_large_values(outliers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a26f7",
   "metadata": {},
   "source": [
    "# Separating Dataset into independent and depandent feature dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'target_column' is the name of your target variable column\n",
    "X = large_val_df.drop(columns=['is_churn'])  # Independent variables (features)\n",
    "y = large_val_df['is_churn']  # Dependent variable (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9aff70",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features(X, y, k=10, score_func=f_classif):\n",
    "    '''\n",
    "    Selects the best features for a classification problem using SelectKBest.\n",
    "\n",
    "    Args:\n",
    "        X (pandas.DataFrame): The DataFrame containing the independent variables.\n",
    "        y (pandas.Series): The Series containing the dependent variable.\n",
    "        k (int): The number of top features to select. Default is 10.\n",
    "        score_func (callable): The scoring function to use for feature selection. Default is f_classif.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of selected feature names.\n",
    "    '''\n",
    "    # Initialize SelectKBest with the specified scoring function and k\n",
    "    kb = SelectKBest(score_func=score_func, k=k)\n",
    "    \n",
    "    # Fit SelectKBest to the data\n",
    "    kb.fit(X, y)\n",
    "    \n",
    "    # Get the scores and feature indices\n",
    "    scores = kb.scores_\n",
    "    indices = np.arange(len(scores))\n",
    "    \n",
    "    # Remove features with NaN scores\n",
    "    non_nan_indices = indices[~np.isnan(scores)]\n",
    "    non_nan_scores = scores[~np.isnan(scores)]\n",
    "\n",
    "    # Get the indices of the selected features\n",
    "    selected_indices = non_nan_indices[np.argsort(non_nan_scores)[::-1][:k]]\n",
    "\n",
    "    # Get the names and scores of the selected features\n",
    "    selected_features = [X.columns[i] for i in selected_indices]\n",
    "    selected_scores = scores[selected_indices]\n",
    "    \n",
    "    # Print the statistical report of the selected features\n",
    "    report = pd.DataFrame({'Feature': selected_features, 'Score': selected_scores})\n",
    "    print(report.sort_values(by='Score', ascending=False))\n",
    "    \n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(report['Feature'], report['Score'], color='b')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = select_best_features(X,y,k=10,score_func=f_classif)\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected_df = X[selected_features[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91546413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot(X, y):\n",
    "    \"\"\"\n",
    "    Generate a heatmap to visualize the correlation between features (X) and target variable (y).\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame): Features DataFrame.\n",
    "    - y (Series): Target variable Series.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function concatenates the features DataFrame (X) and target variable Series (y) to create a combined DataFrame.\n",
    "    It then calculates the correlation matrix between features and the target variable.\n",
    "    Finally, it plots a heatmap to visualize the correlations, with annotations showing the correlation coefficients.\n",
    "    The colormap 'coolwarm' is used to represent positive and negative correlations, and values are formatted to two decimal places.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate X and y to create a DataFrame\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap between X and y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "correlation_plot(X_selected_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d5230",
   "metadata": {},
   "source": [
    "# Train Test Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, test_size=0.2, val_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame or array-like): The feature matrix.\n",
    "        y (Series or array-like): The target variable.\n",
    "        test_size (float or int): The proportion of the dataset to include in the test split.\n",
    "        val_size (float or int): The proportion of the dataset to include in the validation split.\n",
    "        random_state (int or None): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the following splits: X_train, X_val, X_test, y_train, y_val, y_test.\n",
    "    \"\"\"\n",
    "    print(\"Splitting dataset into training, validation, and test sets...\")\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    print(f\"Dataset split into training set ({len(X_train)} samples) and test set ({len(X_test)} samples)\")\n",
    "    \n",
    "    # Split the remaining training set into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size / (1 - test_size),\n",
    "                                                      random_state=random_state)\n",
    "    print(f\"Training set further split into training set ({len(X_train)} samples) and validation set ({len(X_val)} samples)\")\n",
    "    \n",
    "    print(\"Splitting complete!\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X, y, test_size=0.2, val_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and X_test are your feature matrices\n",
    "\n",
    "# Normalize features using MinMaxScaler or StandardScaler\n",
    "scaler = MinMaxScaler()  # or StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f33ebc",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c105df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_neural_network(X_train, y_train, X_test, y_test, epochs=50, batch_size=64, validation_split=0.2, verbose=1):\n",
    "    \"\"\"\n",
    "    Train and evaluate a feedforward neural network for binary classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (array-like): Training data features.\n",
    "    - y_train (array-like): Training data labels.\n",
    "    - X_test (array-like): Test data features.\n",
    "    - y_test (array-like): Test data labels.\n",
    "    - epochs (int): Number of epochs for training. Default is 50.\n",
    "    - batch_size (int): Batch size for training. Default is 64.\n",
    "    - validation_split (float): Fraction of training data to be used as validation data. Default is 0.2.\n",
    "    - verbose (int): Verbosity mode during training and evaluation. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    The function trains a feedforward neural network with ReLU activation in the hidden layers\n",
    "    and sigmoid activation in the output layer. It uses binary cross-entropy loss function,\n",
    "    Adam optimizer, and early stopping with a patience of 5 epochs to prevent overfitting.\n",
    "    After training, it evaluates the model on the test data and prints performance metrics\n",
    "    including accuracy, precision, recall, F1 score, confusion matrix, and classification report.\n",
    "    It also plots the ROC curve and training/validation loss curves to visualize model performance\n",
    "    and checks for overfitting by comparing validation loss trends.\n",
    "    \"\"\"\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model\n",
    "    model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))  # Input layer\n",
    "    model.add(Dense(units=64, activation='relu'))  # Hidden layer\n",
    "    model.add(Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=verbose)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stopping], verbose=verbose)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred_proba)\n",
    "\n",
    "    # Calculate log loss\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    print(f'Log Loss: {logloss:.4f}')\n",
    "\n",
    "    # Calculate other performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Check for overfitting\n",
    "    if np.any(np.diff(history.history['val_loss']) > 0):\n",
    "        print(\"The model is overfitting.\")\n",
    "    else:\n",
    "        print(\"The model is not overfitting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3042133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_neural_network(X_train_normalized, y_train, X_test_normalized, y_test, epochs=50, batch_size=64, validation_split=0.2, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
