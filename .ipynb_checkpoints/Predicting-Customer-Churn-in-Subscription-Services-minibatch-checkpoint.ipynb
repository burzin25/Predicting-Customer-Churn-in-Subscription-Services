{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a868f20",
   "metadata": {},
   "source": [
    "# Feature Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e262504",
   "metadata": {},
   "source": [
    "### Impoting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d38e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burzi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Numerical computing\n",
    "import pandas as pd # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt # Data visualization\n",
    "import seaborn as sns # Advanced data visualization\n",
    "import sklearn # Machine learning algorithms and tools\n",
    "import tensorflow as tf # Deep learning framework\n",
    "import keras # High-level neural networks API\n",
    "import statsmodels.api as sm # Statistical modeling\n",
    "import scipy.stats as stats # Scientific computing\n",
    "import plotly.express as px # Interactive data visualization\n",
    "import nltk # Natural language processing\n",
    "import xgboost as xgb # Gradient boosting library\n",
    "import lightgbm as lgb # Gradient boosting framework\n",
    "import catboost as cb # Gradient boosting on decision trees\n",
    "import imblearn # Library for handling imbalanced datasets\n",
    "# import eli5 # Explanation of machine learning models\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import shap # Interpretability and explainability of models\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, roc_curve, auc\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4b0bb",
   "metadata": {},
   "source": [
    "### Datasets (CSV Files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00254d8f",
   "metadata": {},
   "source": [
    "### Join all the dataframes into one dataframe and read the exported joined dataset \"undersampled_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "804612d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = pd.read_csv('undersampled_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a882007",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a31b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    '''\n",
    "    Perform feature engineering on the provided DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input DataFrame containing the original features.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with additional engineered features.\n",
    "    '''\n",
    "\n",
    "    # Extract year, month, and day from registration_init_time\n",
    "    data['registration_year'] = data['registration_init_time'].dt.year\n",
    "    data['registration_month'] = data['registration_init_time'].dt.month\n",
    "    data['registration_day'] = data['registration_init_time'].dt.day\n",
    "\n",
    "    # Calculate subscription duration\n",
    "    data['subscription_duration'] = (data['membership_expire_date'] - data['registration_init_time']).dt.days\n",
    "\n",
    "    # Extract month and day of the week from date\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "    # Calculate listening session frequency\n",
    "    session_count = data.groupby('msno')['date'].count().reset_index()\n",
    "    session_count.columns = ['msno', 'session_count']\n",
    "    data = pd.merge(data, session_count, on='msno', how='left')\n",
    "\n",
    "    # Calculate average listening time per session\n",
    "    data['avg_listen_time_per_session'] = data['total_secs'] / data['session_count']\n",
    "\n",
    "#     # Calculate ratio of skipped songs\n",
    "#     data['skipped_ratio'] = (data['num_25'] + data['num_50']) / data['num_unq']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2d1b26",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the 'featurization' function to preprocess the DataFrame 'preprocessed_df' \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# by creating additional features based on existing data, and store the result in 'featurized'.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m featurized_df \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mfeature_engineering\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mPerform feature engineering on the provided DataFrame.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    pandas.DataFrame: The DataFrame with additional engineered features.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Extract year, month, and day from registration_init_time\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistration_year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregistration_init_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m     14\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistration_month\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistration_init_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[0;32m     15\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistration_day\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistration_init_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5987\u001b[0m ):\n\u001b[0;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py:580\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_period_dtype(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Apply the 'featurization' function to preprocess the DataFrame 'preprocessed_df' \n",
    "# by creating additional features based on existing data, and store the result in 'featurized'.\n",
    "featurized_df = feature_engineering(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_v2(data):\n",
    "    '''\n",
    "    Perform preprocessing on the provided DataFrame by dropping specified columns and rearranging the columns.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed DataFrame.\n",
    "    '''\n",
    "\n",
    "    # Drop specified columns\n",
    "    columns_to_drop = [\"msno\", \"registration_init_time\", \"transaction_date\", \"membership_expire_date\", \"date\"]\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Rearrange columns to place \"is_churn\" at the end\n",
    "    churn_column = data.pop(\"is_churn\")\n",
    "    data[\"is_churn\"] = churn_column\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_v2_df = preprocessing_v2(featurized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d4a20",
   "metadata": {},
   "source": [
    "# Outliers and Extra Large Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ad9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Drop rows containing outliers in each column of a DataFrame using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame to drop outliers from.\n",
    "    - threshold: float, optional (default=1.5)\n",
    "        The threshold multiplier for determining outliers. A higher threshold will result in fewer outliers being detected.\n",
    "\n",
    "    Returns:\n",
    "    - df_cleaned: DataFrame\n",
    "        A new DataFrame with rows containing outliers removed.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df = drop_outliers(preprocessed_v2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_large_values(X, large_threshold=1e6):\n",
    "    \"\"\"\n",
    "    Drop rows containing very large values from the feature matrix X.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy.ndarray\n",
    "        The feature matrix.\n",
    "    - large_threshold: float, optional (default=1e6)\n",
    "        The threshold for defining very large values.\n",
    "\n",
    "    Returns:\n",
    "    - X_cleaned: numpy.ndarray\n",
    "        The feature matrix with rows containing very large values removed.\n",
    "    \"\"\"\n",
    "    # Check for very large values\n",
    "    large_rows = np.any(np.abs(X) > large_threshold, axis=1)\n",
    "    if np.any(large_rows):\n",
    "        print(\"Rows with very large values found in X. Dropping...\")\n",
    "        X_cleaned = X[~large_rows]  # Drop rows with large values\n",
    "    else:\n",
    "        X_cleaned = X.copy()  # If no large values found, return a copy of X\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_val_df = drop_rows_with_large_values(outliers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a26f7",
   "metadata": {},
   "source": [
    "# Separating Dataset into independent and depandent feature dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'target_column' is the name of your target variable column\n",
    "X = large_val_df.drop(columns=['is_churn'])  # Independent variables (features)\n",
    "y = large_val_df['is_churn']  # Dependent variable (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9aff70",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features(X, y, k=10, score_func=f_classif):\n",
    "    '''\n",
    "    Selects the best features for a classification problem using SelectKBest.\n",
    "\n",
    "    Args:\n",
    "        X (pandas.DataFrame): The DataFrame containing the independent variables.\n",
    "        y (pandas.Series): The Series containing the dependent variable.\n",
    "        k (int): The number of top features to select. Default is 10.\n",
    "        score_func (callable): The scoring function to use for feature selection. Default is f_classif.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of selected feature names.\n",
    "    '''\n",
    "    # Initialize SelectKBest with the specified scoring function and k\n",
    "    kb = SelectKBest(score_func=score_func, k=k)\n",
    "    \n",
    "    # Fit SelectKBest to the data\n",
    "    kb.fit(X, y)\n",
    "    \n",
    "    # Get the scores and feature indices\n",
    "    scores = kb.scores_\n",
    "    indices = np.arange(len(scores))\n",
    "    \n",
    "    # Remove features with NaN scores\n",
    "    non_nan_indices = indices[~np.isnan(scores)]\n",
    "    non_nan_scores = scores[~np.isnan(scores)]\n",
    "\n",
    "    # Get the indices of the selected features\n",
    "    selected_indices = non_nan_indices[np.argsort(non_nan_scores)[::-1][:k]]\n",
    "\n",
    "    # Get the names and scores of the selected features\n",
    "    selected_features = [X.columns[i] for i in selected_indices]\n",
    "    selected_scores = scores[selected_indices]\n",
    "    \n",
    "    # Print the statistical report of the selected features\n",
    "    report = pd.DataFrame({'Feature': selected_features, 'Score': selected_scores})\n",
    "    print(report.sort_values(by='Score', ascending=False))\n",
    "    \n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(report['Feature'], report['Score'], color='b')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = select_best_features(X,y,k=10,score_func=f_classif)\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected_df = X[selected_features[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91546413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot(X, y):\n",
    "    \"\"\"\n",
    "    Generate a heatmap to visualize the correlation between features (X) and target variable (y).\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame): Features DataFrame.\n",
    "    - y (Series): Target variable Series.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function concatenates the features DataFrame (X) and target variable Series (y) to create a combined DataFrame.\n",
    "    It then calculates the correlation matrix between features and the target variable.\n",
    "    Finally, it plots a heatmap to visualize the correlations, with annotations showing the correlation coefficients.\n",
    "    The colormap 'coolwarm' is used to represent positive and negative correlations, and values are formatted to two decimal places.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate X and y to create a DataFrame\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap between X and y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "correlation_plot(X_selected_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d5230",
   "metadata": {},
   "source": [
    "# Train Test Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, test_size=0.2, val_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame or array-like): The feature matrix.\n",
    "        y (Series or array-like): The target variable.\n",
    "        test_size (float or int): The proportion of the dataset to include in the test split.\n",
    "        val_size (float or int): The proportion of the dataset to include in the validation split.\n",
    "        random_state (int or None): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the following splits: X_train, X_val, X_test, y_train, y_val, y_test.\n",
    "    \"\"\"\n",
    "    print(\"Splitting dataset into training, validation, and test sets...\")\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    print(f\"Dataset split into training set ({len(X_train)} samples) and test set ({len(X_test)} samples)\")\n",
    "    \n",
    "    # Split the remaining training set into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size / (1 - test_size),\n",
    "                                                      random_state=random_state)\n",
    "    print(f\"Training set further split into training set ({len(X_train)} samples) and validation set ({len(X_val)} samples)\")\n",
    "    \n",
    "    print(\"Splitting complete!\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X, y, test_size=0.2, val_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and X_test are your feature matrices\n",
    "\n",
    "# Normalize features using MinMaxScaler or StandardScaler\n",
    "scaler = MinMaxScaler()  # or StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f33ebc",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c105df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_neural_network(X_train, y_train, X_test, y_test, epochs=50, batch_size=64, validation_split=0.2, verbose=1):\n",
    "    \"\"\"\n",
    "    Train and evaluate a feedforward neural network for binary classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (array-like): Training data features.\n",
    "    - y_train (array-like): Training data labels.\n",
    "    - X_test (array-like): Test data features.\n",
    "    - y_test (array-like): Test data labels.\n",
    "    - epochs (int): Number of epochs for training. Default is 50.\n",
    "    - batch_size (int): Batch size for training. Default is 64.\n",
    "    - validation_split (float): Fraction of training data to be used as validation data. Default is 0.2.\n",
    "    - verbose (int): Verbosity mode during training and evaluation. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    The function trains a feedforward neural network with ReLU activation in the hidden layers\n",
    "    and sigmoid activation in the output layer. It uses binary cross-entropy loss function,\n",
    "    Adam optimizer, and early stopping with a patience of 5 epochs to prevent overfitting.\n",
    "    After training, it evaluates the model on the test data and prints performance metrics\n",
    "    including accuracy, precision, recall, F1 score, confusion matrix, and classification report.\n",
    "    It also plots the ROC curve and training/validation loss curves to visualize model performance\n",
    "    and checks for overfitting by comparing validation loss trends.\n",
    "    \"\"\"\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model\n",
    "    model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))  # Input layer\n",
    "    model.add(Dense(units=64, activation='relu'))  # Hidden layer\n",
    "    model.add(Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=verbose)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stopping], verbose=verbose)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred_proba)\n",
    "\n",
    "    # Calculate log loss\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    print(f'Log Loss: {logloss:.4f}')\n",
    "\n",
    "    # Calculate other performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Check for overfitting\n",
    "    if np.any(np.diff(history.history['val_loss']) > 0):\n",
    "        print(\"The model is overfitting.\")\n",
    "    else:\n",
    "        print(\"The model is not overfitting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3042133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_neural_network(X_train_normalized, y_train, X_test_normalized, y_test, epochs=25, batch_size=64, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c07253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
